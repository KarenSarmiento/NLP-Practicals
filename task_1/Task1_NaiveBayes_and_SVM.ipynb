{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "#import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import subprocess\n",
    "import statistics\n",
    "from nltk.stem import PorterStemmer\n",
    "import operator as op\n",
    "from functools import reduce\n",
    "from decimal import Decimal\n",
    "from scipy.stats import binom\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_FOLDS = 10\n",
    "DATA_SIZE = 2000\n",
    "POS_PROB = 0.5\n",
    "NEG_PROB = 1 - POS_PROB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN FILES\n",
    "def get_data_set(stemming, presence, bigrams, uni_and_bigrams, cutoff):\n",
    "    \"\"\"Returns a dict mapping fold number to data fold.\n",
    "    \n",
    "    Returns:\n",
    "        fold number (int) -> list of documents \n",
    "        (list of list of strings)\n",
    "    \"\"\"\n",
    "    pos_path = 'POS-tokenized/POS/*.tag'\n",
    "    neg_path = 'NEG-tokenized/NEG/*.tag'\n",
    "    pos_data = _read_data(pos_path)\n",
    "    neg_data = _read_data(neg_path)\n",
    "\n",
    "    # split data into 10 stratified folds\n",
    "    data_set = {}\n",
    "    for i in range(NUM_FOLDS):\n",
    "        data_set[i] = []\n",
    "    \n",
    "    feature_count = 0\n",
    "    for i, docs in enumerate(zip(pos_data, neg_data)):\n",
    "        pos_doc, neg_doc = docs\n",
    "        if stemming:\n",
    "            porter_stemmer = PorterStemmer()\n",
    "            pos_doc = _apply_stemming(porter_stemmer, pos_doc)\n",
    "            neg_doc = _apply_stemming(porter_stemmer, neg_doc)        \n",
    "        if bigrams:\n",
    "            bi_pos_doc = _unigrams_to_bigrams(pos_doc)\n",
    "            bi_neg_doc = _unigrams_to_bigrams(neg_doc)\n",
    "            if uni_and_bigrams:\n",
    "                pos_doc = pos_doc + bi_pos_doc\n",
    "                neg_doc = neg_doc + bi_neg_doc\n",
    "            else:\n",
    "                pos_doc = bi_pos_doc\n",
    "                neg_doc = bi_neg_doc\n",
    "        if cutoff > 0:\n",
    "            pos_doc = _apply_feature_cutoff(pos_doc, cutoff)\n",
    "            neg_doc = _apply_feature_cutoff(neg_doc, cutoff)\n",
    "        if presence:\n",
    "            pos_doc = set(pos_doc)\n",
    "            neg_doc = set(neg_doc)\n",
    "            \n",
    "        data_set[i%NUM_FOLDS].append((pos_doc, \"POS\"))\n",
    "        data_set[i%NUM_FOLDS].append((neg_doc, \"NEG\"))\n",
    "        \n",
    "        feature_count += len(pos_doc) + len(pos_doc)\n",
    "    print(\"feature_count = {}\".format(feature_count))\n",
    "    return data_set\n",
    "\n",
    "def _read_data(folder_path):\n",
    "    files = glob.glob(folder_path)\n",
    "    data = []\n",
    "    for file_name in files:\n",
    "        with open(file_name) as fp:\n",
    "            document = [word.strip(\"\\n\") for word in fp.readlines()]\n",
    "            data.append(document)\n",
    "    return data\n",
    "\n",
    "def _apply_stemming(porter_stemmer, doc):\n",
    "    return [porter_stemmer.stem(word) for word in doc]\n",
    "\n",
    "def _unigrams_to_bigrams(doc):\n",
    "    return [word1 + word2 \n",
    "            for word1, word2 \n",
    "            in zip(doc[:-1], doc[1:])]\n",
    "\n",
    "def _apply_feature_cutoff(doc, cutoff):\n",
    "    token_count = dict()\n",
    "    for word in doc:\n",
    "        if word not in token_count:\n",
    "            token_count[word] = 0\n",
    "        token_count[word] = token_count[word] + 1\n",
    "    for word in doc:\n",
    "        if token_count[word] < cutoff:\n",
    "            doc.remove(word)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_data_set(data_set):\n",
    "    \"\"\" Yields data set organised into training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        data_set: a dict mapping fold number to fold data.\n",
    "        \n",
    "    Returns: \n",
    "        A tuple (training_set, test_set). Each of these entries\n",
    "        represents a list of docs (a list of list of strings).\n",
    "    \"\"\"\n",
    "    for test_num in data_set.keys():\n",
    "        training_set = []\n",
    "        test_set = []\n",
    "        for fold_num, curr_fold in data_set.items():\n",
    "            if fold_num == test_num:\n",
    "                test_set = curr_fold\n",
    "            else:\n",
    "                training_set.extend(curr_fold)\n",
    "        yield (training_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_NB(training_set, test_set):\n",
    "    \"\"\" Classifies docs in test_set based on training_set.\n",
    "    \n",
    "    Args:\n",
    "        training_set: training docs (list of list of strings).\n",
    "        test_set: testing docs (list of list of strings).\n",
    "        \n",
    "    Returns:\n",
    "        A list of the outcomes for each doc in the test_set.\n",
    "    \"\"\"\n",
    "    log_probs = calculate_log_probs(training_set)\n",
    "    correct_classifications = []\n",
    "    for test_doc in test_set:\n",
    "        pos_sum = POS_PROB\n",
    "        neg_sum = NEG_PROB\n",
    "        test_doc_words, true_sentiment = test_doc\n",
    "        for word in test_doc_words:\n",
    "            if word not in log_probs:\n",
    "                continue\n",
    "            pos_log_prob, neg_log_prob = log_probs[word]\n",
    "            pos_sum += pos_log_prob\n",
    "            neg_sum += neg_log_prob\n",
    "        result_sentiment = \"POS\"\n",
    "        if pos_sum < neg_sum:\n",
    "            result_sentiment = \"NEG\"    \n",
    "        \n",
    "        if result_sentiment == true_sentiment:\n",
    "            correct_classifications.append(1)\n",
    "        else:\n",
    "            correct_classifications.append(0)\n",
    "    return correct_classifications\n",
    "\n",
    "def calculate_log_probs(training_set):\n",
    "    \"\"\"Returns logged probabilities of each word\n",
    "    \n",
    "    P(f_i|c) = count(f_i,c)/sum_i count(f_i,c)\n",
    "    \n",
    "    Args:\n",
    "        training_set: documents to be trained on; list of \n",
    "            (list of words, sentiment (string)).\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping words (string) to a \n",
    "        (pos log prob (float), neg log prob (float)) tuple.\n",
    "    \"\"\"\n",
    "    # Find word_counts: word -> (pos_count, neg_count)\n",
    "    word_counts = {}\n",
    "    total_positive_words = 0\n",
    "    total_negative_words = 0\n",
    "    for doc_words, sentiment in training_set:\n",
    "        for word in doc_words:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = (1, 1) # laplace smoothing\n",
    "                total_positive_words += 1\n",
    "                total_negative_words += 1\n",
    "            pos_count, neg_count = word_counts[word]\n",
    "            if sentiment == \"POS\":\n",
    "                word_counts[word] = (pos_count + 1, neg_count)\n",
    "                total_positive_words += 1\n",
    "            else:\n",
    "                word_counts[word] = (pos_count, neg_count + 1)\n",
    "                total_negative_words += 1\n",
    "    \n",
    "    # Log values and find total positive and negative counts.\n",
    "    log_probs = {}\n",
    "    for word, counts in word_counts.items():\n",
    "        pos_counts, neg_counts = counts\n",
    "        pos_log_prob = math.log(float(pos_counts)/total_positive_words)\n",
    "        neg_log_prob = math.log(float(neg_counts)/total_negative_words)\n",
    "        log_probs[word] = (pos_log_prob, neg_log_prob)\n",
    "    return log_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_naive_bayes(stemming, presence, bigrams, uni_and_bigrams, cutoff): \n",
    "    \"\"\"Run naive bayes on data set and print results.\"\"\"\n",
    "    data_set = get_data_set(stemming, presence, bigrams, uni_and_bigrams, cutoff)\n",
    "    organised_sets = cross_validate_data_set(data_set)\n",
    "    correct_classifications = []\n",
    "    accuracies = []\n",
    "    for training_set, test_set in organised_sets:\n",
    "        fold_classifications = classify_NB(training_set, test_set)\n",
    "        correct_classifications.append(fold_classifications)\n",
    "        accuracy = fold_classifications.count(1)/len(fold_classifications)\n",
    "        accuracies.append(accuracy)\n",
    "    return (accuracies, correct_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM_LIGHT\n",
    "def write_data_set_to_files(data_set, fp, word_encodings):\n",
    "    new_word_count = len(word_encodings) +1\n",
    "    for doc_words, sentiment in data_set:\n",
    "        target = \"1\" if sentiment == \"POS\" else \"-1\"\n",
    "        word_counts = dict()\n",
    "        for word in doc_words:\n",
    "            if word not in word_encodings:\n",
    "                word_encodings[word] = new_word_count\n",
    "                new_word_count += 1\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 0\n",
    "            word_counts[word] = word_counts[word] + 1\n",
    "        feature_values = [(word_encodings[word], count)\n",
    "                         for (word, count) in word_counts.items()]\n",
    "        feature_values.sort()\n",
    "        feature_values = [\"{}:{}\".format(encoding, count) \n",
    "                          for (encoding, count) in feature_values]\n",
    "        feature_values = \" \".join(feature_values)\n",
    "        line = \"{} {}\\n\".format(target, feature_values)\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(stemming, presence, bigrams, uni_and_bigrams, cutoff):\n",
    "    data_set = get_data_set(stemming, presence, bigrams, uni_and_bigrams, cutoff)\n",
    "    organised_sets = cross_validate_data_set(data_set)\n",
    "    all_classifications = []\n",
    "    accuracies = []\n",
    "    for training_set, test_set in organised_sets:\n",
    "        word_encodings = dict()\n",
    "        # Train\n",
    "        with open(\"training_set.txt\", \"w+\") as training_fp:\n",
    "            write_data_set_to_files (training_set, \n",
    "                                     training_fp,\n",
    "                                     word_encodings)\n",
    "        subprocess.call([\"svm_light/svm_learn\", \n",
    "                         \"training_set.txt\", \n",
    "                         \"model_file.txt\"])\n",
    "        # Classify\n",
    "        with open(\"test_set.txt\", \"w+\") as test_fp:\n",
    "            write_data_set_to_files (test_set, \n",
    "                                     test_fp,\n",
    "                                     word_encodings)\n",
    "        subprocess.call([\"svm_light/svm_classify\",\n",
    "                         \"test_set.txt\",\n",
    "                         \"model_file.txt\",\n",
    "                         \"predictions.txt\"])\n",
    "        # Test\n",
    "        fold_classifications = []\n",
    "        with open(\"predictions.txt\") as pred_fp:\n",
    "            for test_doc in test_set:\n",
    "                result = float(pred_fp.readline())\n",
    "                test_doc_words, true_sentiment = test_doc\n",
    "                if result >= 0 and true_sentiment == \"POS\" or result < 0 and true_sentiment == \"NEG\":\n",
    "                    fold_classifications.append(1)\n",
    "                else:\n",
    "                    fold_classifications.append(0)\n",
    "        all_classifications.append(fold_classifications)\n",
    "        accuracies.append(fold_classifications.count(1)/len(fold_classifications))\n",
    "    return accuracies, all_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sign_test(base_classifications, new_classifications):\n",
    "    \"\"\"Runs sign test.\n",
    "    \n",
    "    Args:\n",
    "        base_classifications: list of 1s or 0s. 1 if correctly classified and 0 otherwise.\n",
    "        new_classifications: list of 1s or 0s. 1 if correctly classified and 0 otherwise.\n",
    "    \n",
    "    Returns:\n",
    "        p-value.\n",
    "    \"\"\"\n",
    "    better_count = 0\n",
    "    worse_count = 0\n",
    "    equal_count = 0\n",
    "    for base_class, new_class in zip(base_classifications, new_classifications):\n",
    "        if base_class == new_class:\n",
    "            equal_count += 1\n",
    "        elif new_class > base_class:\n",
    "            better_count += 1\n",
    "        else:\n",
    "            worse_count += 1\n",
    "    return _sign_test_aux(better_count, worse_count, equal_count)\n",
    "    \n",
    "def _sign_test_aux(better_count, worse_count, equal_count):\n",
    "    N = 2*math.ceil(equal_count/2) + better_count + worse_count\n",
    "    k = math.ceil(equal_count/2) + min(better_count, worse_count)\n",
    "    return 2 * binom.cdf(k, N, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_test_on_multiple_folds(stemming=False, presence=False, bigrams=False, uni_and_bigrams=False, cutoff=0):    \n",
    "    nb_accuracies, nb_classifications = run_naive_bayes(stemming, presence, bigrams, uni_and_bigrams, cutoff)\n",
    "    svm_accuracies, svm_classifications = run_svm(stemming, presence, bigrams, uni_and_bigrams, cutoff)\n",
    "    \n",
    "    nb_average = sum(nb_accuracies) / len(nb_accuracies)\n",
    "    svm_average = sum(svm_accuracies) / len(svm_accuracies)\n",
    "    print(\"NB = {}\\nAverage = {}\\nSVM={}\\nAverage = {}\".format(nb_accuracies, nb_average, svm_accuracies, svm_average))\n",
    "    \n",
    "    return nb_classifications, svm_classifications\n",
    "\n",
    "\n",
    "def _marked_list_to_p_value(class_1_marked, class_2_marked):\n",
    "    c1_flattened = _flatten_list(class_1_marked)\n",
    "    c2_flattened = _flatten_list(class_2_marked)\n",
    "    p_value = float(_sign_test(c1_flattened, c2_flattened))\n",
    "    print(\"p-value = {}\".format(p_value))\n",
    "    \n",
    "\n",
    "def _flatten_list(list_of_lists):\n",
    "    return [val for sublist in list_of_lists for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)\n",
      "feature_count = 1656748\n",
      "feature_count = 1656748\n",
      "NB = [0.855, 0.845, 0.815, 0.8, 0.78, 0.805, 0.86, 0.8, 0.85, 0.835]\n",
      "Average = 0.8244999999999999\n",
      "SVM=[0.705, 0.775, 0.76, 0.76, 0.725, 0.695, 0.765, 0.69, 0.72, 0.74]\n",
      "Average = 0.7335\n",
      "\n",
      "(2)\n",
      "feature_count = 685018\n",
      "feature_count = 685018\n",
      "NB = [0.86, 0.84, 0.795, 0.795, 0.8, 0.84, 0.84, 0.81, 0.845, 0.835]\n",
      "Average = 0.826\n",
      "SVM=[0.88, 0.855, 0.86, 0.815, 0.835, 0.855, 0.87, 0.86, 0.815, 0.865]\n",
      "Average = 0.851\n",
      "\n",
      "(3)\n",
      "feature_count = 1398912\n",
      "feature_count = 1398912\n",
      "NB = [0.885, 0.885, 0.845, 0.865, 0.82, 0.835, 0.875, 0.865, 0.87, 0.875]\n",
      "Average = 0.8620000000000001\n",
      "SVM=[0.85, 0.84, 0.84, 0.845, 0.825, 0.79, 0.87, 0.81, 0.83, 0.86]\n",
      "Average = 0.836\n",
      "\n",
      "(4)\n",
      "feature_count = 2036328\n",
      "feature_count = 2036328\n",
      "NB = [0.885, 0.88, 0.84, 0.845, 0.83, 0.84, 0.88, 0.84, 0.875, 0.885]\n",
      "Average = 0.86\n",
      "SVM=[0.905, 0.875, 0.88, 0.89, 0.88, 0.85, 0.9, 0.865, 0.85, 0.88]\n",
      "Average = 0.8775000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"(1)\")\n",
    "nb_1, svm_1 = sign_test_on_multiple_folds(stemming=True, presence=False, bigrams=False, uni_and_bigrams=False, cutoff=0)\n",
    "print(\"\\n(2)\")\n",
    "nb_2, svm_2 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=False, uni_and_bigrams=False, cutoff=0)\n",
    "print(\"\\n(3)\")\n",
    "nb_3, svm_3 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=True, uni_and_bigrams=False, cutoff=0)\n",
    "print(\"\\n(4)\")\n",
    "nb_4, svm_4 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=True, uni_and_bigrams=True, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)\n",
      "feature_count = 1656748\n",
      "feature_count = 1656748\n",
      "NB = [0.855, 0.845, 0.815, 0.8, 0.78, 0.805, 0.86, 0.8, 0.85, 0.835]\n",
      "Average = 0.8244999999999999\n",
      "SVM=[0.705, 0.775, 0.76, 0.76, 0.725, 0.695, 0.765, 0.69, 0.72, 0.74]\n",
      "Average = 0.7335\n",
      "\n",
      "(2)\n",
      "feature_count = 685018\n",
      "feature_count = 685018\n",
      "NB = [0.86, 0.84, 0.795, 0.795, 0.8, 0.84, 0.84, 0.81, 0.845, 0.835]\n",
      "Average = 0.826\n",
      "SVM=[0.88, 0.855, 0.86, 0.815, 0.835, 0.855, 0.87, 0.86, 0.815, 0.865]\n",
      "Average = 0.851\n"
     ]
    }
   ],
   "source": [
    "print(\"(1)\")\n",
    "nb_1, svm_1 = sign_test_on_multiple_folds(stemming=True, presence=False, bigrams=False, uni_and_bigrams=False, cutoff=0)\n",
    "print(\"\\n(2)\")\n",
    "nb_2, svm_2 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=False, uni_and_bigrams=False, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nb_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-af5d502d5afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nb_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_marked_list_to_p_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_marked_list_to_p_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_marked_list_to_p_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_marked_list_to_p_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_0' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"nb_0\")\n",
    "_marked_list_to_p_value(nb_0, svm_0)\n",
    "_marked_list_to_p_value(nb_0, svm_1)\n",
    "_marked_list_to_p_value(nb_0, svm_2)\n",
    "_marked_list_to_p_value(nb_0, svm_3)\n",
    "_marked_list_to_p_value(nb_0, svm_4)\n",
    "\n",
    "print(\"\\nnb_1\")\n",
    "_marked_list_to_p_value(nb_1, svm_0)\n",
    "_marked_list_to_p_value(nb_1, svm_1)\n",
    "_marked_list_to_p_value(nb_1, svm_2)\n",
    "_marked_list_to_p_value(nb_1, svm_3)\n",
    "_marked_list_to_p_value(nb_1, svm_4)\n",
    "\n",
    "print(\"\\nnb_2\")\n",
    "_marked_list_to_p_value(nb_2, svm_0)\n",
    "_marked_list_to_p_value(nb_2, svm_1)\n",
    "_marked_list_to_p_value(nb_2, svm_2)\n",
    "_marked_list_to_p_value(nb_2, svm_3)\n",
    "_marked_list_to_p_value(nb_2, svm_4)\n",
    "\n",
    "print(\"\\nnb_3\")\n",
    "_marked_list_to_p_value(nb_3, svm_0)\n",
    "_marked_list_to_p_value(nb_3, svm_1)\n",
    "_marked_list_to_p_value(nb_3, svm_2)\n",
    "_marked_list_to_p_value(nb_3, svm_3)\n",
    "_marked_list_to_p_value(nb_3, svm_4)\n",
    "\n",
    "print(\"\\nnb_4\")\n",
    "_marked_list_to_p_value(nb_4, svm_0)\n",
    "_marked_list_to_p_value(nb_4, svm_1)\n",
    "_marked_list_to_p_value(nb_4, svm_2)\n",
    "_marked_list_to_p_value(nb_4, svm_3)\n",
    "_marked_list_to_p_value(nb_4, svm_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _marked_list_to_p_value(svm_2, svm_3)\n",
    "# _marked_list_to_p_value(svm_2, svm_4)\n",
    "\n",
    "# _marked_list_to_p_value(svm_3, svm_4)\n",
    "\n",
    "_marked_list_to_p_value(svm_1, svm_2)\n",
    "_marked_list_to_p_value(nb_1, nb_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"(1)\")\n",
    "nb_1, svm_1 = sign_test_on_multiple_folds(stemming=True, presence=False, bigrams=False, uni_and_bigrams=False, cutoff=2)\n",
    "print(\"\\n(2)\")\n",
    "nb_2, svm_2 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=False, uni_and_bigrams=False, cutoff=2)\n",
    "print(\"\\n(3)\")\n",
    "nb_3, svm_3 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=True, uni_and_bigrams=False, cutoff=2)\n",
    "print(\"\\n(4)\")\n",
    "nb_4, svm_4 = sign_test_on_multiple_folds(stemming=True, presence=True, bigrams=True, uni_and_bigrams=True, cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
