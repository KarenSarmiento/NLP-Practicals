{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics\n",
    "import subprocess\n",
    "\n",
    "from __future__ import print_function\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.stats import binom\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_FOLDS = 10\n",
    "ACL_PATH = \"aclImdb_v1/aclImdb/{}/*\"\n",
    "ACL_FOLDER_PATHS = [ACL_PATH.format(\"train/unsup\"),\n",
    "                    ACL_PATH.format(\"train/pos\"),\n",
    "                    ACL_PATH.format(\"train/neg\"),\n",
    "                    ACL_PATH.format(\"test/pos\"),\n",
    "                    ACL_PATH.format(\"test/neg\")]\n",
    "PANG_POS_PATH = \"POS-tokenized/POS/*\"\n",
    "PANG_NEG_PATH = \"NEG-tokenized/NEG/*\"\n",
    "SEED = 0\n",
    "NUM_FOLDS = 10\n",
    "POS = 1\n",
    "NEG = -1\n",
    "R = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_docs_from_folder(folder_path, pretokenised):\n",
    "    files = glob.glob(folder_path)\n",
    "    folder_documents = []\n",
    "    for file_name in files:\n",
    "        with open(file_name) as fp:\n",
    "            if pretokenised:\n",
    "                doc_words = [word.strip(\"\\n\") for word in fp.readlines()]\n",
    "                folder_documents.append(doc_words)\n",
    "            else:\n",
    "                # TODO: Use tokeniser: https://www.nltk.org/api/nltk.tokenize.html\n",
    "                document = fp.read()\n",
    "                doc_words = document.split(\" \")\n",
    "                folder_documents.append(doc_words)\n",
    "    return folder_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_training_set(folder_paths_list):\n",
    "    all_docs = []\n",
    "    for folder_path in folder_paths_list:\n",
    "        folder_documents = _get_docs_from_folder(folder_path, \n",
    "                                                 pretokenised=False)\n",
    "        all_docs.extend(folder_documents)\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pang_dataset(pos_path, neg_path, stemming, presence, bigrams, cutoff):\n",
    "    pos_data = _get_docs_from_folder(pos_path, pretokenised=True)\n",
    "    neg_data = _get_docs_from_folder(neg_path, pretokenised=True)\n",
    "    \n",
    "    validation_set = []\n",
    "    test_and_training_docs = []\n",
    "    for i, pos_and_neg_doc in enumerate(zip(pos_data, neg_data)):\n",
    "        pos_doc, neg_doc = pos_and_neg_doc\n",
    "        # Apply any transformations.\n",
    "        if stemming:\n",
    "            porter_stemmer = PorterStemmer()\n",
    "            pos_doc = _apply_stemming(porter_stemmer, pos_doc)\n",
    "            neg_doc = _apply_stemming(porter_stemmer, neg_doc)        \n",
    "        if bigrams:\n",
    "            pos_doc = _unigrams_to_bigrams(pos_doc)\n",
    "            neg_doc = _unigrams_to_bigrams(neg_doc)\n",
    "        if cutoff > 0:\n",
    "            pos_doc = _apply_feature_cutoff(pos_doc, cutoff)\n",
    "            neg_doc = _apply_feature_cutoff(neg_doc, cutoff)\n",
    "        if presence:\n",
    "            pos_doc = set(pos_doc)\n",
    "            neg_doc = set(neg_doc)\n",
    "        \n",
    "        # Obtain validation set.\n",
    "        if i%10 == 0:\n",
    "            validation_set.append((pos_doc, POS))\n",
    "            validation_set.append((neg_doc, NEG))\n",
    "        else:\n",
    "            test_and_training_docs.append((pos_doc, neg_doc))\n",
    "        \n",
    "    # Split remaining data into folds.\n",
    "    data_set = {}\n",
    "    for fold_index in range(NUM_FOLDS):\n",
    "        data_set[fold_index] = []\n",
    "    for i, (pos_doc, neg_doc) in enumerate(test_and_training_docs):\n",
    "        data_set[i%NUM_FOLDS].append((pos_doc, POS))\n",
    "        data_set[i%NUM_FOLDS].append((neg_doc, NEG))\n",
    "    return validation_set, data_set\n",
    "\n",
    "def _apply_stemming(porter_stemmer, doc):\n",
    "    return [porter_stemmer.stem(word) for word in doc]\n",
    "\n",
    "def _unigrams_to_bigrams(doc):\n",
    "    return [word1 + word2 \n",
    "            for word1, word2 \n",
    "            in zip(doc[:-1], doc[1:])]\n",
    "\n",
    "def _apply_feature_cutoff(doc, cutoff):\n",
    "    token_count = dict()\n",
    "    for word in doc:\n",
    "        if word not in token_count:\n",
    "            token_count[word] = 0\n",
    "        token_count[word] = token_count[word] + 1\n",
    "    for word in doc:\n",
    "        if token_count[word] < cutoff:\n",
    "            doc.remove(word)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(training_set, epochs=10):\n",
    "    documents = [TaggedDocument(doc, [i]) \n",
    "                 for i, doc in enumerate(training_set)]\n",
    "    model = Doc2Vec(documents, seed=SEED, dbow_words=1, \n",
    "                    epochs=epochs, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_docs_as_embeddings(dataset, doc2vec_model, name_of_file):\n",
    "    with open(name_of_file, \"w+\") as fp:\n",
    "        for document in dataset:\n",
    "            doc_words, sentiment = document\n",
    "            feature_vector = doc2vec_model.infer_vector(doc_words)\n",
    "            feature_values = [\"{}:{}\".format(index + 1, value)\n",
    "                              for index, value \n",
    "                              in enumerate(feature_vector)]\n",
    "            line = \"{} {}\\n\".format(sentiment, \n",
    "                                  \" \".join(feature_values))\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(training_set, test_set, doc2vec_model):\n",
    "    training_file_name = \"training_embeddings.txt\"\n",
    "    test_file_name = \"test_embeddings.txt\"\n",
    "    model_file_name = \"model_file.txt\"\n",
    "    predictions_file_name = \"predictions.txt\"\n",
    "    \n",
    "    if (doc2vec_model == None):\n",
    "        print(\"ERROR: SVM with counts not yet supported!\")\n",
    "        print(\"Please spcify model.\")\n",
    "        return\n",
    "    else:\n",
    "        store_docs_as_embeddings(training_set, doc2vec_model,\n",
    "                                 training_file_name)\n",
    "        store_docs_as_embeddings(test_set, doc2vec_model, \n",
    "                                 test_file_name)\n",
    "    # TRAIN\n",
    "    subprocess.call([\"svm_light/svm_learn\", \n",
    "                     training_file_name, \n",
    "                     model_file_name])\n",
    "    \n",
    "    # CLASSIFY\n",
    "    subprocess.call([\"svm_light/svm_classify\",\n",
    "                     test_file_name,\n",
    "                     model_file_name,\n",
    "                     predictions_file_name])\n",
    "    \n",
    "    # TEST\n",
    "    results = []\n",
    "    with open(predictions_file_name) as pred_fp:\n",
    "        for test_doc in test_set:\n",
    "            doc_words, doc_sentiment = test_doc\n",
    "            result = float(pred_fp.readline())\n",
    "            if np.sign(result) == doc_sentiment:\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(baseline_results, system_results, subset_count=R):\n",
    "    \"\"\"Runs the permutation test on two sets of results.\n",
    "    \n",
    "    Args:\n",
    "        baseline_results: The classification results for the \n",
    "            baseline.\n",
    "        system_results: The classification results for the new \n",
    "            system. The files should be in the same order as\n",
    "            in baseline_results.\n",
    "    \n",
    "    Returns:\n",
    "        p-value.\n",
    "    \"\"\"\n",
    "    if len(baseline_results) != len(system_results):\n",
    "        print(\"Need equal number of results for perm test!\")\n",
    "        return\n",
    "    \n",
    "    # Get mean difference of true results.\n",
    "    base_mean_difference = _mean_difference(\n",
    "        baseline_results, system_results)\n",
    "    \n",
    "    # Generate random sample indices to swap.\n",
    "    test_indices = [random.randint(0, len(baseline_results) -1)\n",
    "                   for _ in range(subset_count)]\n",
    "    \n",
    "    # Count number of times swapping leads to a greater or equal\n",
    "    # mean.\n",
    "    exceeded_base_count = 0\n",
    "    for test_index in test_indices:\n",
    "        flip = random.choice([True, False])\n",
    "        if flip:\n",
    "            new_mean_difference = _mean_difference(\n",
    "                baseline_results, system_results, swap=test_index)\n",
    "            if new_mean_difference >= base_mean_difference:\n",
    "                exceeded_base_count += 1\n",
    "        else:\n",
    "            exceeded_base_count += 1\n",
    "    \n",
    "    # Return probability.\n",
    "    return (exceeded_base_count + 1)/float(subset_count + 1)\n",
    "        \n",
    "        \n",
    "def _mean_difference(results_1, results_2, swap=-1):\n",
    "    mean_1 = 0\n",
    "    mean_2 = 0\n",
    "    for i, (val_1, val_2) in enumerate(zip(results_1, results_2)):\n",
    "        if swap == i:\n",
    "            mean_1 += val_2\n",
    "            mean_2 += val_1\n",
    "        else:\n",
    "            mean_1 += val_1\n",
    "            mean_2 += val_2\n",
    "    mean_1 /= float(len(results_1))\n",
    "    mean_2 /= float(len(results_2))\n",
    "    return abs(mean_1 - mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(results):\n",
    "    return statistics.mean(results) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(data, dimensions):\n",
    "    return TSNE(n_components=dimensions).fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACL training set...\n",
      "Fetched 100000 docs.\n",
      "Loading Pang validation set...\n",
      "Fetched 200 validation docs and \n",
      "10 folds with 180 docs each.\n"
     ]
    }
   ],
   "source": [
    "# GET DATA SETS\n",
    "print(\"Loading ACL training set...\")\n",
    "embeddings_training_set = get_embeddings_training_set(\n",
    "    ACL_FOLDER_PATHS)\n",
    "print(\"Fetched {} docs.\".format(len(embeddings_training_set)))\n",
    "\n",
    "print(\"Loading Pang validation set...\")\n",
    "validation_set, pang_folds = get_pang_dataset(\n",
    "    PANG_POS_PATH, PANG_NEG_PATH, \n",
    "    stemming=False, presence=False, bigrams=False, cutoff=0)\n",
    "print(\"Fetched {} validation docs and \".format(len(validation_set)))\n",
    "print(\"{} folds with {} docs each.\".format(len(pang_folds), \n",
    "                                            len(pang_folds[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec model...\n",
      "Doc2Vec(dm/m,d100,n5,w5,mc5,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN DOC2VEC MODEL\n",
    "print(\"Training Doc2Vec model...\")\n",
    "doc2vec_model = train_doc2vec_model(embeddings_training_set,\n",
    "                                    epochs=20)\n",
    "print(doc2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on validation set...\n",
      "The accuracy is 83.0%.\n"
     ]
    }
   ],
   "source": [
    "# TEST ON VALIDATION SET\n",
    "print(\"Running on validation set...\")\n",
    "temp_training_set = [doc for fold in pang_folds.values()\n",
    "                    for doc in fold]\n",
    "results = run_svm(temp_training_set, validation_set, \n",
    "                   doc2vec_model)\n",
    "print(\"The accuracy is {}%.\".format(accuracy(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### //TODO:\n",
    "* Find visualisations for vectors.\n",
    "* Come up with interesting research question.\n",
    "* Finetune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0252141 , -0.2148532 , -0.09877743, ..., -0.4085281 ,\n",
       "        -1.3435767 ,  1.4234582 ],\n",
       "       [ 0.10215303,  0.8025779 ,  0.18199776, ..., -1.5692176 ,\n",
       "        -0.09126595,  0.7739603 ],\n",
       "       [-0.1941799 ,  0.03760209, -0.13399512, ..., -0.3956676 ,\n",
       "         0.5957031 ,  0.9362498 ],\n",
       "       ...,\n",
       "       [ 0.5121043 ,  0.20250937, -0.58231807, ...,  0.44705144,\n",
       "         0.02350491, -0.11774922],\n",
       "       [-0.08252161, -0.2356661 ,  0.22111419, ...,  0.23112664,\n",
       "        -0.05226233,  0.02298038],\n",
       "       [-0.1857906 , -0.30076313,  0.0235896 , ...,  0.2357155 ,\n",
       "         0.02402979,  0.16078818]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.wv.index2word\n",
    "doc2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.wv.vectors[:200]\n",
    "# reduced_vectors = reduce_dimensions(doc2vec_model.wv.vectors[:200], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "xs = [1,2,3,4,5,6]\n",
    "ys = [1,2,3,4,5,6]\n",
    "zs = [1,2,3,4,5,6]\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
