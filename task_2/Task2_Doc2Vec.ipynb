{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import glob\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from __future__ import print_function\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.stats import binom\n",
    "\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_FOLDS = 10\n",
    "ACL_PATH = \"aclImdb_v1/aclImdb/{}/*\"\n",
    "ACL_FOLDER_PATHS = [ACL_PATH.format(\"train/unsup\"),\n",
    "                    ACL_PATH.format(\"train/pos\"),\n",
    "                    ACL_PATH.format(\"train/neg\"),\n",
    "                    ACL_PATH.format(\"test/pos\"),\n",
    "                    ACL_PATH.format(\"test/neg\")]\n",
    "PANG_POS_PATH = \"POS-tokenized/POS/*\"\n",
    "PANG_NEG_PATH = \"NEG-tokenized/NEG/*\"\n",
    "SEED = 0\n",
    "NUM_FOLDS = 10\n",
    "POS = 1\n",
    "NEG = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_docs_from_folder(folder_path, pretokenised):\n",
    "    files = glob.glob(folder_path)\n",
    "    folder_documents = []\n",
    "    for file_name in files:\n",
    "        with open(file_name) as fp:\n",
    "            if pretokenised:\n",
    "                doc_words = [word.strip(\"\\n\") for word in fp.readlines()]\n",
    "                folder_documents.append(doc_words)\n",
    "            else:\n",
    "                # TODO: Use tokeniser: https://www.nltk.org/api/nltk.tokenize.html\n",
    "                document = fp.read()\n",
    "                doc_words = document.split(\" \")\n",
    "                folder_documents.append(doc_words)\n",
    "    return folder_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_training_set(folder_paths_list):\n",
    "    all_docs = []\n",
    "    for folder_path in folder_paths_list:\n",
    "        folder_documents = _get_docs_from_folder(folder_path, \n",
    "                                                 pretokenised=False)\n",
    "        all_docs.extend(folder_documents)\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pang_dataset(pos_path, neg_path, stemming, presence, bigrams, cutoff):\n",
    "    pos_data = _get_docs_from_folder(pos_path, pretokenised=True)\n",
    "    neg_data = _get_docs_from_folder(neg_path, pretokenised=True)\n",
    "    \n",
    "    validation_set = []\n",
    "    test_and_training_docs = []\n",
    "    for i, pos_and_neg_doc in enumerate(zip(pos_data, neg_data)):\n",
    "        pos_doc, neg_doc = pos_and_neg_doc\n",
    "        # Apply any transformations.\n",
    "        if stemming:\n",
    "            porter_stemmer = PorterStemmer()\n",
    "            pos_doc = _apply_stemming(porter_stemmer, pos_doc)\n",
    "            neg_doc = _apply_stemming(porter_stemmer, neg_doc)        \n",
    "        if bigrams:\n",
    "            pos_doc = _unigrams_to_bigrams(pos_doc)\n",
    "            neg_doc = _unigrams_to_bigrams(neg_doc)\n",
    "        if cutoff > 0:\n",
    "            pos_doc = _apply_feature_cutoff(pos_doc, cutoff)\n",
    "            neg_doc = _apply_feature_cutoff(neg_doc, cutoff)\n",
    "        if presence:\n",
    "            pos_doc = set(pos_doc)\n",
    "            neg_doc = set(neg_doc)\n",
    "        \n",
    "        # Obtain validation set.\n",
    "        if i%10 == 0:\n",
    "            validation_set.append((pos_doc, POS))\n",
    "            validation_set.append((neg_doc, NEG))\n",
    "        else:\n",
    "            test_and_training_docs.append((pos_doc, neg_doc))\n",
    "        \n",
    "    # Split remaining data into folds.\n",
    "    data_set = {}\n",
    "    for fold_index in range(NUM_FOLDS):\n",
    "        data_set[fold_index] = []\n",
    "    for i, (pos_doc, neg_doc) in enumerate(test_and_training_docs):\n",
    "        data_set[i%NUM_FOLDS].append((pos_doc, POS))\n",
    "        data_set[i%NUM_FOLDS].append((neg_doc, NEG))\n",
    "    return validation_set, data_set\n",
    "\n",
    "def _apply_stemming(porter_stemmer, doc):\n",
    "    return [porter_stemmer.stem(word) for word in doc]\n",
    "\n",
    "def _unigrams_to_bigrams(doc):\n",
    "    return [word1 + word2 \n",
    "            for word1, word2 \n",
    "            in zip(doc[:-1], doc[1:])]\n",
    "\n",
    "def _apply_feature_cutoff(doc, cutoff):\n",
    "    token_count = dict()\n",
    "    for word in doc:\n",
    "        if word not in token_count:\n",
    "            token_count[word] = 0\n",
    "        token_count[word] = token_count[word] + 1\n",
    "    for word in doc:\n",
    "        if token_count[word] < cutoff:\n",
    "            doc.remove(word)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(training_set, epochs=10):\n",
    "    documents = [TaggedDocument(doc, [i]) \n",
    "                 for i, doc in enumerate(training_set)]\n",
    "    model = Doc2Vec(documents, seed=SEED, dbow_words=1, \n",
    "                    epochs=epochs, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_docs_as_embeddings(dataset, doc2vec_model, name_of_file):\n",
    "    with open(name_of_file, \"w+\") as fp:\n",
    "        for document in dataset:\n",
    "            doc_words, sentiment = document\n",
    "            feature_vector = doc2vec_model.infer_vector(doc_words)\n",
    "            feature_values = [\"{}:{}\".format(index + 1, value)\n",
    "                              for index, value \n",
    "                              in enumerate(feature_vector)]\n",
    "            line = \"{} {}\\n\".format(sentiment, \n",
    "                                  \" \".join(feature_values))\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(training_set, test_set, doc2vec_model):\n",
    "    training_file_name = \"training_embeddings.txt\"\n",
    "    test_file_name = \"test_embeddings.txt\"\n",
    "    model_file_name = \"model_file.txt\"\n",
    "    predictions_file_name = \"predictions.txt\"\n",
    "    \n",
    "    if (doc2vec_model == None):\n",
    "        print(\"ERROR: SVM with counts not yet supported!\")\n",
    "        print(\"Please spcify model.\")\n",
    "        return\n",
    "    else:\n",
    "        store_docs_as_embeddings(training_set, doc2vec_model,\n",
    "                                 training_file_name)\n",
    "        store_docs_as_embeddings(test_set, doc2vec_model, \n",
    "                                 test_file_name)\n",
    "    # TRAIN\n",
    "    subprocess.call([\"svm_light/svm_learn\", \n",
    "                     training_file_name, \n",
    "                     model_file_name])\n",
    "    \n",
    "    # CLASSIFY\n",
    "    subprocess.call([\"svm_light/svm_classify\",\n",
    "                     test_file_name,\n",
    "                     model_file_name,\n",
    "                     predictions_file_name])\n",
    "    \n",
    "    # TEST\n",
    "    accuracy_count = 0\n",
    "    with open(predictions_file_name) as pred_fp:\n",
    "        for test_doc in test_set:\n",
    "            doc_words, doc_sentiment = test_doc\n",
    "            result = float(pred_fp.readline())\n",
    "            if np.sign(result) == doc_sentiment:\n",
    "                accuracy_count += 1\n",
    "    return float(accuracy_count) / len(test_set) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACL training set...\n",
      "Fetched 100000 docs.\n",
      "Loading Pang validation set...\n",
      "Fetched 200 validation docs and \n",
      "10 folds with 180 docs each.\n"
     ]
    }
   ],
   "source": [
    "# GET DATA SETS\n",
    "print(\"Loading ACL training set...\")\n",
    "embeddings_training_set = get_embeddings_training_set(\n",
    "    ACL_FOLDER_PATHS)\n",
    "print(\"Fetched {} docs.\".format(len(embeddings_training_set)))\n",
    "\n",
    "print(\"Loading Pang validation set...\")\n",
    "validation_set, pang_folds = get_pang_dataset(\n",
    "    PANG_POS_PATH, PANG_NEG_PATH, \n",
    "    stemming=False, presence=False, bigrams=False, cutoff=0)\n",
    "print(\"Fetched {} validation docs and \".format(len(validation_set)))\n",
    "print(\"{} folds with {} docs each.\".format(len(pang_folds), \n",
    "                                            len(pang_folds[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec model...\n",
      "Doc2Vec(dm/m,d100,n5,w5,mc5,s0.001,t3)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN DOC2VEC MODEL\n",
    "print(\"Training Doc2Vec model...\")\n",
    "doc2vec_model = train_doc2vec_model(embeddings_training_set,\n",
    "                                    epochs=20)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on validation set...\n",
      "The accuracy is 82.5%.\n"
     ]
    }
   ],
   "source": [
    "# TEST ON VALIDATION SET\n",
    "print(\"Running on validation set...\")\n",
    "temp_training_set = [doc for fold in pang_folds.values()\n",
    "                    for doc in fold]\n",
    "accuracy = run_svm(temp_training_set, validation_set, \n",
    "                   doc2vec_model)\n",
    "print(\"The accuracy is {}%.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### //TODO:\n",
    "* Implement permutation test.\n",
    "* Find visualisations for vectors.\n",
    "* Come up with interesting research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
